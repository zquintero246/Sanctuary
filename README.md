# üß† Sanctuary ‚Äì Pipeline de Voz Full Duplex

Sanctuary ahora incluye una orquestaci√≥n **E2E en streaming** que escucha, razona y responde sobre la marcha. El ciclo completo admite:

- **STT** con parciales cada ~150‚ÄØms, final con heur√≠stica de endpointing y *token timings* cuando el backend los soporta.
- **LLM** con `generate_stream()` token a token.
- **TTS** que reproduce audio en chunks (100‚Äì200‚ÄØms) con `stop()` inmediato para *barge-in*.
- **Tracer** que expone latencias (`stt_first_partial_ms`, `llm_first_token_ms`, ‚Ä¶) en formato JSON.
- **Cliente de micr√≥fono** que env√≠a audio 16‚ÄØkHz en vivo y reproduce la respuesta del asistente.

---

## üîÅ Flujo de extremo a extremo

```mermaid
graph LR
    A[Micr√≥fono] -->|20-40‚ÄØms PCM| B[WhisperStreamingSTT]
    B -->|stt_partial| C[Orchestrator]
    C -->|prompt| D[TransformersStreamingLLM]
    D -->|tokens| C
    C -->|texto| E[CoquiStreamingTTS]
    E -->|audio chunks| F[Cliente voz]
    C --> G[Tracer]
    G -->|metrics| F
    F -->|voz usuario durante SPEAKING| C
```

El objetivo es entregar las primeras palabras del asistente en **<‚ÄØ800‚ÄØms** y turnos completos <‚ÄØ1.2‚ÄØs.

---

## üöÄ Puesta en marcha

1. **Preparar entorno**

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

   > Requisitos adicionales: `ffmpeg` para Whisper y dependencias del modelo Coqui XTTS (la primera ejecuci√≥n descargar√° los pesos).

2. **Configurar modelos (opcional)**

   Variables de entorno disponibles:

   | Variable | Descripci√≥n | Default |
   | --- | --- | --- |
   | `SANCTUARY_STT_MODEL` | Tama√±o del modelo Whisper (`tiny`, `base`, `small`, ‚Ä¶) | `small` |
   | `SANCTUARY_STT_LANGUAGE` | ISO 639-1 para forzar idioma | `es` |
   | `SANCTUARY_LLM_MODEL` | HuggingFace model id (causal LM) | `distilgpt2` |
   | `SANCTUARY_LLM_SYSTEM_PREFIX` | Prefijo de estilo para el prompt | `""` |
   | `SANCTUARY_TTS_MODEL` | Modelo Coqui TTS | `tts_models/multilingual/multi-dataset/xtts_v2` |
   | `SANCTUARY_TTS_LANGUAGE` | Idioma de s√≠ntesis | `es` |
   | `SANCTUARY_TTS_SPEAKER_WAV` | Ruta a audio para *voice cloning* | `None` |

3. **Levantar el servidor WebSocket**

   ```bash
    uvicorn main:app --host 0.0.0.0 --port 8000 --reload
   ```

4. **Conectar el cliente de micr√≥fono**

   ```bash
   python voice_client.py --print-events
   ```

   El cliente captura audio mono 16‚ÄØkHz en bloques de 20‚ÄØms, imprime parciales STT / m√©tricas y reproduce los chunks de TTS que env√≠a el servidor.

---

## üåê Protocolo `/voice`

- **Cliente ‚Üí Servidor (binario):** PCM `int16` mono 16‚ÄØkHz, bloques de 20‚Äì40‚ÄØms.
- **Servidor ‚Üí Cliente:**

  ```json
  {"type": "tts_metadata", "sample_rate": 24000}
  {"type": "stt_partial", "text": "hola es", "is_final": false}
  {"type": "stt_final", "text": "hola, ¬øest√°s ah√≠?", "is_final": true}
  {"type": "assistant_text", "text": "¬°Hola! S√≠, te escucho."}
  {"type": "metrics", "stt_first_partial_ms": 180, "llm_first_token_ms": 220, "tts_first_audio_ms": 140, "turn_total_ms": 980}
  ```

- **Audio TTS:** frames binarios PCM (`int16`) enviados como mensajes WS binarios. El cliente reajusta autom√°ticamente la frecuencia usando `tts_metadata`.
- **Fin de turno opcional:** `{"type": "end_user_turn"}`.

---

## üß© Componentes relevantes

- `Services/sanctuary_core/interfaces.py` ‚Äì contratos de STT/LLM/TTS/VAD.
- `Services/sanctuary_core/orchestrator.py` ‚Äì estados `LISTENING ‚Üí THINKING ‚Üí SPEAKING`, barge-in y colas de audio.
- `Services/sanctuary_core/tracer.py` ‚Äì utilidades `mark()` y `span()` + c√°lculo de m√©tricas.
- `Services/sanctuary_core/llm_transformers.py` ‚Äì adaptador HuggingFace con `TextIteratorStreamer`.
- `Services/sanctuary_stt/whisper_streaming.py` ‚Äì Whisper en streaming con parciales y finales.
- `Services/sanctuary_tts/coqui_streaming.py` ‚Äì s√≠ntesis XTTS v2 troceada para streaming.
- `voice_client.py` ‚Äì CLI que env√≠a audio del micr√≥fono y reproduce la respuesta.

---

## üìä Telemetr√≠a

El orquestador utiliza `Tracer` para registrar eventos con `time.perf_counter()` y, al cerrar el turno, env√≠a `{"type": "metrics", ‚Ä¶}` al cliente. Tambi√©n imprime en stdout un arreglo JSON con el timeline completo, √∫til para dashboards o exportar a observabilidad.

---

## üß™ Pruebas autom√°ticas

La suite cubre escenarios de parciales STT, arranque temprano de TTS, *barge-in* y emisi√≥n de m√©tricas.

```bash
pytest
```

---

## üõ£Ô∏è Pr√≥ximos pasos sugeridos

- A√±adir *jitter buffer* configurable en el cliente (actualmente usa reproducci√≥n directa).
- Integrar almacenamiento de contexto conversacional y memoria a largo plazo.
- Instrumentar OpenTelemetry y dashboards de latencia por etapa.
- A√±adir fallback de modelos ligeros para hardware sin GPU.

---

## üìÑ Licencia

Proyecto licenciado bajo Apache 2.0. Consulta `LICENSE` para m√°s detalles.
