# ðŸ§  Sanctuary â€“ Voice Streaming Pipeline

Sanctuary ahora expone una **pipeline de voz casi en tiempo real** con escucha, razonamiento y respuesta en modo *full duplex controlado*. El sistema recibe audio continuo, publica parciales de STT, genera tokens de LLM en streaming y sintetiza voz con soporte de *barge-in* y telemetrÃ­a de latencias.

---

## ðŸ” Flujo de extremo a extremo

```mermaid
graph LR
    A[Audio del usuario] -->|20-40ms| B[STT streaming]
    B -->|parciales 100-200ms| C[Orchestrator]
    C --> D[LLM streaming]
    D -->|tokens| C
    C --> E[TTS streaming]
    E -->|chunks 100-200ms| F[ReproducciÃ³n]
    C --> G[Tracer]
    G --> H[Metrics JSON]
    C -->|Barge-in| E
```

1. **LISTENING** â€“ El usuario envÃ­a audio PCM por WebSocket `/voice`.
2. **THINKING** â€“ Cuando el VAD detecta silencio o un posible final de frase, el orquestador construye el prompt y activa `LLM.generate_stream()`.
3. **SPEAKING** â€“ El primer token activa TTS inmediatamente; los chunks de audio se envÃ­an al cliente con un *jitter buffer* de ~150â€¯ms.
4. **Barge-in** â€“ Si el usuario vuelve a hablar, el orquestador detiene TTS (`stop()` + *fade*) y retoma la escucha.
5. **Tracing** â€“ Cada turno registra `stt_first_partial_ms`, `stt_final_ms`, `llm_first_token_ms`, `tts_first_audio_ms` y `turn_total_ms`.

Objetivo de latencias: primeras palabras del asistente en <â€¯800â€¯ms, turno completo <â€¯1.2â€¯s.

---

## ðŸš€ CÃ³mo ejecutar el servidor `/voice`

1. **Instala dependencias** (recomendado usar entorno virtual):

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   pip install fastapi uvicorn pytest  # utilidades del servidor y pruebas
   ```

2. **Inicia el servidor WebSocket**:

   ```bash
   uvicorn main:app --reload --host 0.0.0.0 --port 8000
   ```

3. **ConÃ©ctate al endpoint** `ws://localhost:8000/voice` enviando audio PCM mono 16â€¯kHz (20â€“40â€¯ms por chunk). El servidor responde con:

   ```json
   {"type": "stt_partial", "text": "hola es", "is_final": false}
   {"type": "stt_final", "text": "hola, Â¿estÃ¡s ahÃ­?", "is_final": true}
   {"type": "assistant_text", "text": "Â¡Hola! SÃ­, te escucho."}
   {"type": "metrics", "stt_first_partial_ms": 180, "llm_first_token_ms": 220, "tts_first_audio_ms": 140, "turn_total_ms": 980}
   ```

   El audio de salida se envÃ­a como frames binarios PCM listos para reproducciÃ³n.

> **Nota:** `main.py` usa implementaciones *scripted* para propÃ³sitos de demostraciÃ³n. Sustituye `ScriptedSTT/LLM/TTS` por tus adaptadores reales.

---

## ðŸ§± Componentes clave

- `Services/sanctuary_core/interfaces.py` â€“ contratos para STT/LLM/TTS/VAD.
- `Services/sanctuary_core/orchestrator.py` â€“ coordina estados `IDLE`, `LISTENING`, `THINKING`, `SPEAKING`, `INTERRUPTED`.
- `Services/sanctuary_core/tracer.py` â€“ utilidades `mark()` y `span()` para mÃ©tricas.
- `Services/whisper_stt/streaming.py` â€“ envoltorio genÃ©rico de STT con parciales y endpointing.
- `Services/sanctuary_core/llm.py` â€“ adaptador de LLM con `generate_stream()` token a token.
- `Services/xtts_tts/streaming.py` â€“ TTS con jitter buffer y `stop()` para *barge-in*.
- `Services/sanctuary_core/stubs.py` â€“ implementaciones scriptadas usadas en pruebas y demo.

---

## ðŸ§ª Pruebas automÃ¡ticas

Ejecuta la suite con:

```bash
pytest
```

Las pruebas validan:

1. EmisiÃ³n de parciales STT antes del final.
2. Arranque de TTS con el primer token del LLM.
3. Barge-in deteniendo la sÃ­ntesis.
4. GeneraciÃ³n de mÃ©tricas por turno.

---

## ðŸ“ˆ TelemetrÃ­a

El `Tracer` imprime eventos en stdout como JSON. Al cierre de cada turno, el orquestador envÃ­a un mensaje `metrics` con los campos clave. Esto permite instrumentar dashboards de latencia o persistir mÃ©tricas en logs centralizados.

---

## ðŸ—º PrÃ³ximos pasos sugeridos

- Sustituir los componentes scriptados por integraciones reales (Whisper, modelos LLM, TTS neural).
- AÃ±adir *fade-out* al detener TTS y normalizar niveles de audio.
- Persistir contexto conversacional y aÃ±adir memoria de diÃ¡logos.
- Instrumentar tracing distribuido (OpenTelemetry) y dashboards.

---

## ðŸ“„ Licencia

Este proyecto estÃ¡ licenciado bajo Apache 2.0. Consulta `LICENSE` para mÃ¡s detalles.
